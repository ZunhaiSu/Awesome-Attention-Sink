# Attention-Sink Theory  
|Paper|Conference|Author|
|:---:|:---:|:---:|
|[Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453)|ICLR 2024|MIT|
|[Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration](https://arxiv.org/abs/2406.15765)|ICML2024|Georgia Institute of Technology|
|[When Attention Sink Emerges in Language Models: An Empirical View](https://arxiv.org/abs/2406.15765)|ICLR submission|Sea AI Lab|
|[Spectral Filters, Dark Signals, and Attention Sinks](https://arxiv.org/abs/2402.09221)||FAIR|
|[Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs](https://arxiv.org/abs/2410.13835)|ICLR submission|UC Berkeley|
|[Massive Activations in Large Language Models](https://arxiv.org/abs/2402.17762)|COLM2024|Carnegie Mellon University|
|[Vision Transformers Need Registers](https://arxiv.org/abs/2309.16588)|ICLR2024|FAIR|


# Attention-Sink Applications in Model Compression
|Paper|Conference|Author|
|:---:|:---:|:---:|
|[Prefixing Attention Sinks can Mitigate Activation Outliers for Large Language Model Quantization](https://arxiv.org/abs/2406.12016)|EMNLP2024|POSTECH|
|[SinkQ: Accurate 2-bit KV Cache Quantization with Dynamic Sink Tracking](https://openreview.net/forum?id=bJ33TvbJW0)|ICLR submission||
|[DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads](https://arxiv.org/abs/2410.10819)|ICLR submission|MIT|
|[SirLLM: Streaming Infinite Retentive LLM](https://arxiv.org/abs/2405.12528)||Shanghai Jiao Tong University|
|[Star Attention: Efficient LLM Inference over Long Sequences](https://arxiv.org/abs/2411.17116)|ICLR submission|NVIDIA|
|[MixDQ: Memory-Efficient Few-Step Text-to-Image Diffusion Models with Metric-Decoupled Mixed Precision Quantization](https://link.springer.com/chapter/10.1007/978-3-031-72630-9_17)|ECCV2024|Tsinghua University|
|[RazorAttention: Efficient KV Cache Compression Through Retrieval Heads](https://arxiv.org/pdf/2407.15891)||1Huawei Technologies Co., Ltd|
|[MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression](https://arxiv.org/abs/2406.14909)||Tsinghua University|
|[Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters](https://arxiv.org/abs/2406.12335)|EMNLP2024|Nara Institute of Science and Technology|
|[IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact](https://arxiv.org/abs/2403.01241)|ACL2024|Tsinghua University|
|[DFRot: Achieving Outlier-Free and Massive Activation-Free for Rotated LLMs with Refined Rotation](https://arxiv.org/abs/2412.00648)|ICLR submission|Zhejiang University|
|[SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models](https://arxiv.org/abs/2405.06219)|COLM2024|Shanghai AI Laboratory|
|[KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization](https://arxiv.org/abs/2401.18079)|NeurIPS2024|University of California, Berkeley|
|[PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs](https://arxiv.org/abs/2410.05265)|ICLR submission|The University of Hong Kong|

# Attention-Sink Applications in Enhancing Model Performance
|Paper|Conference|Author|
|:---:|:---:|:---:|
|[Enhanced Structured State Space Models via Grouped FIR Filtering and Attention Sink Mechanisms](https://arxiv.org/abs/2408.00244)||University of Manchester|
|[SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models](https://arxiv.org/abs/2406.05678)|||
|[Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs](https://link.springer.com/chapter/10.1007/978-3-031-73010-8_8)|ECCV2024|Zhejiang University|
|[VASparse: Towards Efficient Visual Hallucination Mitigation for Large Vision-Language Model via Visual-Aware Sparsification](https://arxiv.org/abs/2501.06553)||Peking University|
|[Mamba-R: Vision Mamba ALSO Needs Registers](https://arxiv.org/abs/2405.14858)||Johns Hopkins University|
|[Robustness Tokens: Towards Adversarial Robustness of Transformers](https://eccv.ecva.net/virtual/2024/poster/2297)|ECCV2024|University of Geneva|


# Attention-Sink Application of Others
|Paper|Conference|Author|
|:---:|:---:|:---:|
|[Zero-Shot RTL Code Generation with Attention Sink Augmented Large Language Models](https://arxiv.org/abs/2401.08683)||Ozyegin University|
|[SEED-Story: Multimodal Long Story Generation with Large Language Model](https://arxiv.org/abs/2407.08683)|||
